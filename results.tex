% !TEX root = msc_thesis.tex

\chapter{Results} \label{ch:results}

After making changes to the ELASPIC pipeline that are described in Section \ref{sec:elaspic}, we retrained ELASPIC core and interface predictors and validated them on new data. This involved curating high-quality training, validation and test datasets (Section \ref{sec:datasets}), selecting the best hyperparameters for the machine learning algorithm using grid-search (Section \ref{sec:gridsearch}), selecting the set of most informative features using feature elimination (Section \ref{sec:feature_elimination}), and testing the final predictors on the test datasets to compare their performance with competing methods (Section \ref{sec:validation}).

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{static/elaspic/core_vs_interface_final.png}
	\caption[Example of a core and interface mutation.]{Diagram showing an example of a core (A) and an interface (B) mutation. The effect of core mutations is measured as the change in the Gibbs free energy of protein folding. The effect of interface mutations is measured as the change in the Gibbs free energy of protein-protein interaction. The protein depicted in the diagram is ferricytochrome c from \textit{Rhodospirillum molischianum} (PDB entry \url{2ccy}).}
	\label{fig:core_vs_interface}
\end{figure}


\section{Datasets} \label{sec:datasets}

The ELASPIC pipeline includes two machine learning predictors: a ``core predictor'', which predicts the change in the Gibbs free energy of folding ($\Delta \Delta G_{core}$) caused by mutations, and an ``interface predictor'', which predicts the change in the Gibbs free energy of protein-protein interaction ($\Delta \Delta G_{interface}$) caused by mutations. In order to train, validate and test those predictors, we compiled a number of datasets from different sources, as described in Table \ref{tab:datasets}.

We used the ``Protherm'' dataset to train the core predictor, and the ``Skempi'' dataset to train the interface predictor. For the training datasets, we calculated features describing each mutation using the standalone pipeline (see Section \ref{sec:standalone_pipeline}) and the database pipeline (see Section \ref{sec:standalone_pipeline}) in order to make sure that both pipelines produce similar results and that the trained predictors perform well in both settings. In cases where only the PDB position of the mutation is provided in the dataset, we mapped the PDB position to the corresponding UniProt protein using SIFTS \cite{velankar_sifts:_2013}. For the database pipeline, we attempted to construct four homology models of each domain and domain-domain interaction, with sequence identity to the template structure falling in each of the following bins: less than or equal to 40\% sequence identity, greater than 40\% but less than or equal to 60\% sequence identity, greater than 60\% but less than or equal to 80\% sequence identity and greater than 80\% sequence identity. We expected that including homology models with low sequence identity to the template structures would improve the ability of the predictor to generalize to external datasets, since both the Protherm and the Skempi datasets are over-represented in proteins that have a crystal structure deposited in the PDB.

We used the ``Taipale'' dataset, which measures mutation-induced change in protein stability using a chaperone interaction assay, to validate the core predictor, and the ``Taipale PPI'' and ``Taipale GPCA'' datasets, which measure mutation-induced changes in protein-protein interactions using yeast two-hybrid and \textit{Gaussia princeps} luciferase complementation assays, respectively, to validate the interface predictor \cite{sahni_widespread_2015}. We also selected a subset of mutations from the Humsavar \cite{consortium_uniprot:_2015}, ClinVar \cite{landrum_clinvar:_2016}, and COSMIC \cite{forbes_cosmic:_2015} datasets to validate both core and interface predictors. While mutation deleteriousness and $\Delta \Delta G$ are different metrics, it is expected that deleterious mutations, on average, should have a higher impact on protein structure that benign mutations. Therefore, accurate $\Delta \Delta G$ predictions should have a higher correlation with the deleteriousness score, defined as $1$ for deleterious mutations and $0$ for benign mutations, than inaccurate predictions.

For our test datasets, we used mutations from the Humsavar, ClinVar, and COSMIC datasets affecting proteins that do not appear in \textit{any} of our training or validation datasets. In addition, we used the ``SUMO Ligase'' dataset, which measures the effect of mutations on the activity of SUMO Ligase, the ``AB-Bind'' dataset, which measures the effect of mutations on antibondy binding affinity, and the ``Benedix'' dataset, which measures the effect of mutations on the affinity between $\beta$-lactamase and $\beta$-lactamase-inhibitor. In all cases, the dataset for the core predictor was restricted to mutations that do not fall inside a protein-protein interface, and the dataset for the interface predictor was restricted to mutations that do fall inside a protein-protein interface.

The overlap in mutations found in different datasets is presented in Figures \ref{fig:training_set_overlap_core} and \ref{fig:training_set_overlap_interface}, for core and interface mutations, respectively. We made sure that no mutation from out test sets appears in our training and validation sets.

% \clearpage
\input{datasets.tex}


\clearpage
\section{Hyperparameter optimisation} \label{sec:gridsearch}

ELASPIC uses the gradient boosting regressor (GBR) algorithm, implemented in scikit-learn \cite{scikit-learn}, to  combine over 70 different sequential and structural features and predict the change in the Gibbs free energy change of protein folding or protein-protein interaction. The GBR algorithm was selected because it achieved higher performance than linear regression, support vector machine and random forest algorithms, in 20-fold cross-validation over the training set \cite{berliner_combining_2014}.

In order to select the best set of GBR hyperparameters, we performed exhaustive ``grid-search'', where we measured the performance of the GBR algorithm for 3,600 different combinations of hyperparameters (Table \ref{tab:gridsearch_parameters}). For each set of hyperparameters, we computed the Spearman correlation between predicted and experimental $\Delta \Delta G$ values for mutations in the training set, using 4-fold cross-validation. We also computed the Spearman correlation between predicted $\Delta \Delta G$ values and the experimental values recorded for our validation datasets. We used the combined scores $CS_{core}$ (Equations \ref{eq:combined_score_core}) and $CS_{interface}$ (Equation \ref{eq:combined_score_interface}) to select the best set of hyperparameters for the core and interface predictors, respectively. The contribution of each dataset to the combined score was assigned in an ``ad-hoc'' manner, giving more weight to large datasets than to small datasets, and making sure that the performance on energy-based datasets, including training and Taipale datasets, has a bigger overall impact on the combined score than performance on a deleteriousness-based datasets, such as Humsavar, ClinVar and COSMIC. We used combined scores instead of training set cross-validation alone because we wanted to select predictors that generalize well to external datasets. Since our training sets are limited and biased in the number and type of proteins and protein-protein interactions that they contain, the performance of the predictors on the training set may not be an accurate indicator of their performance in general. Our validation datasets contain many more distinct proteins and protein-protein interactions than our training datasets, and therefore the performance on the validation datasets should be indicative of how well the predictors generalize to other proteins in the human genome.

\begin{equation} \label{eq:combined_score_core}
    CS_{core} = \frac{3 \cdot Cross\_validation + Humsavar + ClinVar + COSMIC + Taipale}{7}
\end{equation}

\begin{equation} \label{eq:combined_score_interface}
    CS_{interface} = \frac{3 \cdot Cross\_validation + Humsavar + ClinVar + COSMIC + \frac{Taipale\_{PPI}}{4} + \frac{Taipale\_{GPCA}}{4}}{6.5}
\end{equation}

In Figures \ref{fig:gridsearch_core} and \ref{fig:gridsearch_interface}, we show the performance of core and interface predictors, for different sets of hyperparameters, sorted by the combined score. For both the core and interface predictors, the performance of the predictor on the training dataset, measured through cross-validation, is highly correlated with its performance on the validation datasets. However, selecting hyperparameters solely based on cross-validation performance would result in a predictor that substantially underperforms on the validation datasets.

\clearpage
\input{gridsearch.tex}


\clearpage
\section{Feature elimination} \label{sec:feature_elimination}

After selecting the best set of hyperparameters for core and interface predictors, we performed feature elimination to evaluate the contribution of each feature to the overall accuracy of the predictor, and to select the sets of features that result in the most accurate predictions.

Feature elimination was performed using the following recursive strategy:

\vspace{-\topsep}
\begin{itemize}
	\itemsep0em
	\item Leave out each feature from the training set, one at a time.
	\item Train the predictor using all but the left out feature.
	\item Calculate the combined score ($CS_{core}$ or $CS_{interface}$) evaluating the performance of the predictor.
	\item Discard the feature that, when left out, produced the predictor with the highest combined score.
	\item Repeat the process until only one feature remains.
\end{itemize}

Performances of the core and interface predictors at every step of feature elimination are shown in Figures \ref{fig:feature_elimination_core} and \ref{fig:feature_elimination_interface}. The sets of features that produced the best-performing predictors are described in Tables \ref{tab:core_features} and \ref{tab:interface_features}.

Most features play a surprisingly small role in the performance of the ELASPIC predictor. In fact, we can achieve near-optimal performance with both core and interface predictors by using only 6 features (displayed in bold in Tables \ref{tab:core_features} and \ref{tab:interface_features}). This suggests either that most features are not informative in predicting the energetic effect of mutations, or that the training set is too noisy for the contribution of those features to make a significant impact on the accuracy of the predictor.

\clearpage
\input{feature_elimination.tex}


\clearpage
\section{Validation} \label{sec:validation}

\subsection{Performance on the training, validation and test datasets}

The final ELASPIC core and interface predictors were trained using the best set of hyperparameters (Section \ref{sec:gridsearch}) and the best set of features (Section \ref{sec:feature_elimination}) that were found for each predictor. Performance of the predictors on the training, validation and test datasets is shown in Figures \ref{fig:core_validation} and \ref{fig:interface_validation}, for core and interface predictors, respectively.

The ELASPIC core predictor outperforms FoldX and Provean on the Taipale dataset, which is the only validation dataset explicitly measuring the effect of mutations on protein stability rather than mutation deleteriousness (Figure \ref{fig:validation_performance_core}). It also outperforms FoldX and Provean on the core subsets of the SUMO and AB-Bind test datasets (Figure \ref{fig:test_performance_core}). The core subsets of those datasets only contain mutations located more than 6 {\AA} away from another chain in the PDB.

The ELASPIC interface predictor also outperforms FoldX and Provean on the Taipale GPCA dataset (Figure \ref{fig:validation_performance_interface}). It performs marginally worse than Provean on the Taipale PPI dataset, but this is likely because the Taipale PPI dataset contains many known deleterious mutations, which are predicted well by Protherm. The ELASPIC interface predictor outperforms Protherm and FoldX on the SUMO Ligase and AB-Bind test datasets (Figure \ref{fig:test_performance_interface}) both alone and in combination with the core predictor. The ELASPIC interface predictor shows slightly lower performance than FoldX on the very small Benedix dataset.

Both the core and interface predictors performs better than FoldX but worse than Provean on the validation and test subsets of the Humsavar, ClinVar and COSMIC (Figures \ref{fig:test_performance_core} and \ref{fig:test_performance_interface}). The low performance of the ELASPIC compared to Provean can be justified, since ELASPIC attempts to model the effect of mutations on protein stability or protein-protein interaction affinity, and does not take into account other reasons that a mutation may be deleterious. For example, mutations can affect the active site or the signal sequence of a protein, which may prove to be highly deleterious to the organism but would have only a marginal effect on protein stability.

\clearpage
\input{validation1.tex}
\clearpage


\subsection{Distinguishing gain-of-function and loss-of-function mutations}

We explored the ability of FoldX, ELASPIC and Provean to differentiate between gain-of-function and loss-of-function mutations by comparing the output of the tools for cancer driver mutations falling inside oncogenes and tumour suppressor genes, respectively (Figures \ref{fig:validation_cancer_score_distribution} and \ref{fig:validation_cancer}). We considered two sets of cancer driver mutations: i) mutations in the COSMIC database that are predicted to be cancer drivers by FATHMM, and ii) mutations in the database of curated mutations (DoCM) \cite{griffith_civic:_2016} that are known to be cancer drivers through \textit{in vitro} and \textit{in vivo} experiments. As a set of oncogenes and a set of tumour suppressor genes, we used genes in the cancer gene census database \cite{futreal_census_2004} that are marked as dominant or recessive, respectively, excluding genes that appear in the Protherm or Skempi datasets. We expected tools that predict the deleteriousness of mutations, such as Provean, to see mutations falling inside oncogenes and tumour suppressor genes as equally deleterious, since both types of mutations can lead to cancer. Conversely, we expected tools that predict the structural impact of mutations, such as FoldX and ELASPIC, to see mutations falling inside tumour suppressor genes as more destabilizing than mutations falling inside oncogenes, since the former, but not the latter, destroy the function of the protein. In fact, FoldX, ELASPIC and Provean all predict that mutations falling inside tumour suppressor genes are more destabilizing or deleterious than mutations falling inside oncogenes (Figure \ref{fig:validation_cancer_score_distribution}). The effect is more pronounced for curated driver mutations in DoCM (Figure \ref{fig:validation_cancer_score_distribution_high_confidence}) than for predicted driver mutations in the COSMIC database (Figure \ref{fig:validation_cancer_score_distribution_full}), likely because the COSMIC dataset contains many passenger mutations that are falsely predicted to be drivers. ELASPIC $\Delta \Delta G$ is the most informative score for predicting whether a mutation falls inside a tumour suppressor gene or an oncogene, as indicated by the areas under the receiver operating characteristic curves (Figures \ref{fig:validation_cancer_full} and \ref{fig:validation_cancer_high_confidence}). However, the advantage of ELASPIC over Provean largely disappears when we evaluate the ability of the tools to predict whether the \textit{protein} is an oncogene or a tumour suppressor gene, using the mean of the scores for all mutations falling in that protein (Figures \ref{fig:validation_cancer_bygene_full} and \ref{fig:validation_cancer_bygene_high_confidence}). This suggests that ELASPIC is better than Provean at scoring mutations in proteins for which both tools make relatively good predictions, but it is not better than Provean for all other mutations.

\clearpage
\input{validation2.tex}

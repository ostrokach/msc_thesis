% !TEX root = msc_thesis.tex

\chapter{Results} \label{chap:results}

After making changes to the ELASPIC pipeline that are described in Chapter \ref{chap:implementation}, we retrained ELASPIC core and interface predictors and validated them on new data.


\section{Datasets}

The datasets that were used to train, validate and test the predictors are described in Table \ref{tab:datasets}. We made sure that no mutations in the test set appear in our training and validation sets (see Figures \ref{fig:training_set_overlap_core} and \ref{fig:training_set_overlap_interface} for core and interface mutations, respectively). In the case of Humsavar, ClinVar and COSMIC datasets, we made sure that no \textit{protein} in the test set appear in the training and validation sets.

Previously, we had seen a low correlation between ELASPIC-predicted $\Delta \Delta G$ and the deleteriousness of mutations. To address this, we split the Humsavar, ClinVar, and COSMIC datasets into validation and test subsets, and used the performance on the validation subset as part of the scoring function for selecting the optimal hyperparameters and number of features.


\section{Hyperparameter optimisation}

We used the $combined\_score_{core}$ and $combined\_score_{interface}$ metrics, given by Equations \ref{eq:combined_score_core} and \ref{eq:combined_score_interface}, respectively, to evaluate the performance of the predictors during hyperparameter optimization and feature elimination. We expected that this would allow us to select a predictor that performs well not only on the training set, but also generalized to other datasets. While mutation deleteriousness and $\Delta \Delta G$ are different metrics, it is expected that deleterious mutations, on average, should have a higher impact on protein structure that benign mutations. Therefore, accurate $\Delta \Delta G$ predictions should have a higher correlation with the deleteriousness score, defined as $1$ for deleterious mutations and $0$ for benign mutations.

The performance on all datasets is correlated.

\begin{equation} \label{eq:combined_score_core}
	\begin{aligned}
    combined\_score_{core} =
		\frac{1}{7} \cdot \Big[
			& 3 \cdot Cross\_validation \\
			& + Humsavar + ClinVar + COSMIC \\
			& + Taipale\Big]
	\end{aligned}
\end{equation}

\begin{equation} \label{eq:combined_score_interface}
	\begin{aligned}
    combined\_score_{interface} =
		\frac{1}{6.5} \cdot \Big[
			& 3 \cdot Cross\_validation \\
			& + Humsavar + ClinVar + COSMIC \\
			& + \frac{1}{4} \cdot \left(Taipale\_{PPI} + Taipale\_{GPCA}\right)\Big]
	\end{aligned}
\end{equation}


\section{Feature elimination}



\section{Validation}

We used a combined score to select the best SGB hyperparameters during grid-search over parameter space, and to select the optimal number of features during feature elimination.


SGB hyperparameters and the number of features to optimize both cross-validation performance on the training set and performance on the validation sets.






output and the validation parts of those datasets throughout hyperparameter optimization (green, red and purple lines in Figures \ref{fig:gridsearch_core} and \ref{fig:gridsearch_interface}) and feature elimination (green, red and purple lines in Figures \ref{fig:feature_elimination_core} and \ref{fig:feature_elimination_interface}). ELASPIC uses the stochastic gradient boosting regression (GBR) algorithm, implemented in scikit-learn \cite{scikit-learn}.



ELASPIC described in output xxx features in total.
1. We calculated those features for the Provean and the Skempi training sets.
2. We removed features that were note different in any of the training cases (xxx for core mutations and yyy for interface mutations).


3. It has been reported that balancing the training set by including both positive and negative samples


As described in [], balancing the training set can significantly improve performance. However, with Provean balancing the training set can bias the result because most mutations are to unconserved amino acids (often alanine) and


Most structural features play a surprisingly small role in the performance of the ELASPIC predictor. In fact, we can achieve near-optimal performance with both core and interface predictors by using only 6 features (displayed in bold in Tables \ref{tab:core_features} and \ref{tab:interface_features}). This suggests either that most features are not informative in predicting the energetic effect of mutations, or that the training set is too noisy for the contribution of those features to make a significant impact on the accuracy of the predictor.

Provean score and, in the case of the core predictor, BLOSUM62 matrix score, where the only sequence-based featurs selected through feature elimination.

We built two core predictors and two interface predictors:

\begin{enumerate}
	\item No sequence features but a balanced training set.
	\item Sequence features but no balanced training set.
\end{enumerate}

- Accuracy over different sequence identity bins

- within protein correlation on the test set



\section{Datasets}



% \begin{figure}[ht!]
% 	\centering
% 	\includegraphics[width=0.65\textwidth]{static/elaspic_training_set/data_statistics/training_set_overlap_data_core.pdf}
% 	\caption[Size and overlap of core and interface datasets.]{Size and overlap of core and interface datasets.}
% \end{figure}



\section{Machine learning}





\clearpage

\section{Predicting mutation-induced changes in the Gibbs free energy of protein folding}

\subsection{Hyperparameter optimization and feature elimination}

\subsection{Validation}




% \clearpage



\section{Predicting mutation-induced changes in the Gibbs free energy of protein-protein interaction}

\subsection{Hyperparameter optimization and feature elimination}

\subsection{Validation}

\clearpage

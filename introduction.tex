% !TEX root = msc_thesis.tex

\chapter{Introduction} \label{ch:introduction}

Advances in DNA sequencing technology have drastically lowered the cost and improved the accuracy of high-throughput sequencing \cite{wetterstrand_dna_2016}. Different sequencing techniques, including RNA-seq, whole-genome sequencing and whole-exome sequencing \cite{ku_exome_2012}, now present as viable and cost-effective tools both in the laboratory, where they permits the study of individual cells and cell populations at an unprecedented level of detail \cite{eberwine_promise_2014}, and in the clinic, where they can assist in the diagnosis and treatment of pediatric diseases \cite{chrystoja_whole_2014} and in the design of targeted therapies against cancer \cite{nik-zainal_landscape_2016}. However, while there has been enormous growth in the amount of genomic data that is generated and the number of sequence variants that are discovered, interpreting this data to produce meaningful and actionable results remains a challenge.

\textit{In vivo} and \textit{in vitro} experimental techniques remain the gold standard for elucidating the effect of DNA sequence variants. However, evaluating experimentally the effect of all discovered variants is not feasible, both in terms of time and resources that would be required. Computational techniques have been developed to predict the effect of different variants and to prioritize them for experimental validation. Those techniques can be loosely divided into three categories: sequence-based tools, structure-based tools, and tools that combine both sequential and structural information.

Sequence-based tools rely on some form of a conservation score, describing the frequency with which a particular nucleotide or amino acid is found at the given position in domain-, protein- or genome-level alignments, in order to make their prediction \cite{ng_sift:_2003,adzhubei_predicting_2001,li_automated_2009,network_integrated_2011,kircher_general_2014,shihab_ranking_2014,choi_predicting_2012}. Due to their speed and scalability, sequence-based tools are the de-facto standard for annotating newly discovered variants. However, they remain limited in their accuracy and the type of information that they can provide \cite{dorfman_common_2010}. In particular, they only predict whether or not a particular mutation is likely to be deleterious, and provide no information as to \textit{why} that mutation may be deleterious. This makes it difficult to act upon those predictions, for example by designing drugs that would curtail the effect of disease-causing mutations or would take advantage of mutations found in cancer.

Structure-based tools predict the effect of mutations on protein structure and / or function using features describing the three-dimensional structure of the protein. They range from accurate but computationally expensive alchemical free energy calculations, which involve modelling the structural transition from the wildtype to the mutant protein and using different integration techniques to calculate the energy of the transition \cite{monticelli_introduction_2013}, to quicker but more approximate techniques, which use semi-empirical or statistical potentials and assume that the backbone of the protein remains fixed \cite{benedix_predicting_2009,pires_mcsm:_2014,laimer_maestro_2015,petukh_predicting_2015}. In theory, structure-based tools should be able to offer more insight into the effect of missense mutations than sequence-based tools, since the effect is caused by changes in protein structure and function and not by changes in DNA sequence. However, since existing structure-based tools require manual setup and a crystal structure of the protein being mutated, there has not been a systematic, genome-wide comparison of the performance of sequence- and structure-based tools in the analysis of mutations.

Several tools have been developed that attempts to combine sequence- and structure-based information in order to make more accurate predictions about the deleteriousness \cite{baugh_robust_2016} and the structural impact \cite{dehouck_fast_2009,berliner_combining_2014,li_mutabind_2016} of mutations. These tools generally are ``meta-predictors'' which integrate the results of several sequence- and structure-based tools using a machine learning algorithms trained on an appropriate dataset. ELASPIC, developed by Niklas Berliner \textit{et al.}, is a particularly interesting example, because, while trained using homology models instead of crystal structures, it still achieves relatively high accuracy in predicting the effect of mutations on protein stability and protein-protein interaction affinity \cite{berliner_combining_2014}. With the growth in the number of crystal structures deposited in the Protein Data Bank \cite{berman_protein_2000}, it is now possible to create homology models of proteins and protein-protein interactions with high coverage of an entire proteome \cite{mosca_interactome3d:_2013}. This suggests that ELASPIC could be extended to work on a genome-wide scale, offering a way to examine the contribution that structural information could make to our analysis and interpretation of mutations.

The aim of this project was to extend ELASPIC so that it could predict the effect of mutations on protein stability and protein-protein interaction affinity on a genome-wide scale. In Chapter \ref{ch:implementation}, we describe modifications that had to be made to ELASPIC and the underlying pipelines in order to make the genome-wide analysis of mutations possible. We also discuss the implementation of the ELASPIC ``jobsubmitter'', which allows ELASPIC to be run through a webserver in a scalable way. In Chapter \ref{ch:results}, we describe the performance of ELASPIC on the training, validation and test datasets, and compare its performance to other sequence- and structure-based tools. In Chapter \ref{ch:discussion}, we discuss the results of this work. In Chapter \ref{ch:future_directions}, we propose several directions for future study.
